{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_WZGQoVqJv6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# === 1. Загрузка данных ===\n",
        "df_control = pd.read_excel('norm.xlsx')\n",
        "df_md = pd.read_excel('dep.xlsx')\n",
        "df_pd = pd.read_excel('sz.xlsx')\n",
        "\n",
        "# Таблица с важными метриками и их важностями\n",
        "important_features_df = pd.read_excel('important.xlsx')\n",
        "\n",
        "# Вытаскиваем только важные признаки\n",
        "important_features = important_features_df['Feature'].tolist()\n",
        "feature_importance = dict(zip(important_features_df['Feature'], important_features_df['normalized importance']))\n",
        "\n",
        "# Подготовка данных\n",
        "df_control['label'] = 'control'\n",
        "df_md['label'] = 'D'\n",
        "df_pd['label'] = 'D'\n",
        "\n",
        "df = pd.concat([df_control, df_md, df_pd], ignore_index=True)\n",
        "\n",
        "X = df[important_features]\n",
        "y = df['label']\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Умножаем каждый признак на его importance\n",
        "for feature in important_features:\n",
        "    X[feature] = X[feature] * feature_importance[feature]\n",
        "\n",
        "# === 2. Деление на трейн/тест ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# === 3. Балансировка классов через SMOTE ===\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# === 4. Масштабирование признаков ===\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# === 5. Объявление моделей ===\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_params = {'n_estimators': [100, 200], 'max_depth': [5, 10, None]}\n",
        "rf_grid = GridSearchCV(rf, rf_params, cv=3, n_jobs=-1)\n",
        "\n",
        "lr = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
        "lr_params = {'C': [0.01, 0.1, 1, 10]}\n",
        "lr_grid = GridSearchCV(lr, lr_params, cv=3, n_jobs=-1)\n",
        "\n",
        "svm = SVC(class_weight='balanced', probability=True, random_state=42)\n",
        "svm_params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "svm_grid = GridSearchCV(svm, svm_params, cv=3, n_jobs=-1)\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb_params = {'n_estimators': [100, 200], 'max_depth': [3, 5, 7]}\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, n_jobs=-1)\n",
        "\n",
        "cat = CatBoostClassifier(verbose=0, random_seed=42)\n",
        "\n",
        "# === 6. Обучение моделей ===\n",
        "models = {\n",
        "    'Random Forest': rf_grid,\n",
        "    'Logistic Regression': lr_grid,\n",
        "    'SVM': svm_grid,\n",
        "    'XGBoost': xgb_grid,\n",
        "    'CatBoost': cat\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# Словарь для хранения F1 для каждого класса отдельно\n",
        "class_f1_scores = {model: [] for model in models}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        model.fit(X_train_scaled, y_train_balanced)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train_balanced, y_train_balanced)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    f1_per_class = f1_score(y_test, y_pred, average=None)  # F1 для каждого класса\n",
        "\n",
        "    results.append((name, acc, f1_macro))\n",
        "    class_f1_scores[name] = f1_per_class  # Сохраняем F1 для каждого класса\n",
        "\n",
        "# === 7. Ансамбли ===\n",
        "# Voting Classifier\n",
        "voting = VotingClassifier(estimators=[\n",
        "    ('rf', rf_grid.best_estimator_),\n",
        "    ('xgb', xgb_grid.best_estimator_),\n",
        "    ('cat', cat)\n",
        "], voting='soft')\n",
        "voting.fit(X_train_balanced, y_train_balanced)\n",
        "voting_pred = voting.predict(X_test)\n",
        "voting_acc = accuracy_score(y_test, voting_pred)\n",
        "voting_f1 = f1_score(y_test, voting_pred, average='macro')\n",
        "results.append(('Voting Ensemble', voting_acc, voting_f1))\n",
        "class_f1_scores['Voting Ensemble'] = f1_score(y_test, voting_pred, average=None)\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking = StackingClassifier(\n",
        "    estimators=[('lr', lr_grid.best_estimator_), ('svm', svm_grid.best_estimator_)],\n",
        "    final_estimator=RandomForestClassifier(random_state=42)\n",
        ")\n",
        "stacking.fit(X_train_scaled, y_train_balanced)\n",
        "stacking_pred = stacking.predict(X_test_scaled)\n",
        "stacking_acc = accuracy_score(y_test, stacking_pred)\n",
        "stacking_f1 = f1_score(y_test, stacking_pred, average='macro')\n",
        "results.append(('Stacking Ensemble', stacking_acc, stacking_f1))\n",
        "class_f1_scores['Stacking Ensemble'] = f1_score(y_test, stacking_pred, average=None)\n",
        "\n",
        "# === 8. Вывод результатов ===\n",
        "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Macro F1-score'])\n",
        "print(results_df)\n",
        "\n",
        "# Вывод F1 для каждого класса отдельно\n",
        "print(\"\\nF1 Score для каждого класса:\")\n",
        "for model, f1_scores in class_f1_scores.items():\n",
        "    print(f\"\\n{model}:\")\n",
        "    for i, f1 in enumerate(f1_scores):\n",
        "        class_name = le.inverse_transform([i])[0]  # Преобразуем индекс в оригинальное имя класса\n",
        "        print(f\"  {class_name}: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4lZ-MEwGMCY",
        "outputId": "c8a9356c-d716-4acb-fc62-8e557d5782ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-171048b642f6>:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[feature] = X[feature] * feature_importance[feature]\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:41:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:41:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Accuracy  Macro F1-score\n",
            "0        Random Forest  0.792135        0.786467\n",
            "1  Logistic Regression  0.780899        0.774925\n",
            "2                  SVM  0.775281        0.768711\n",
            "3              XGBoost  0.780899        0.773100\n",
            "4             CatBoost  0.792135        0.786467\n",
            "5      Voting Ensemble  0.808989        0.803404\n",
            "6    Stacking Ensemble  0.747191        0.742104\n",
            "\n",
            "F1 Score для каждого класса:\n",
            "\n",
            "Random Forest:\n",
            "  D: 0.7516778523489933\n",
            "  control: 0.821256038647343\n",
            "\n",
            "Logistic Regression:\n",
            "  D: 0.738255033557047\n",
            "  control: 0.8115942028985508\n",
            "\n",
            "SVM:\n",
            "  D: 0.7297297297297297\n",
            "  control: 0.8076923076923077\n",
            "\n",
            "XGBoost:\n",
            "  D: 0.7310344827586207\n",
            "  control: 0.8151658767772512\n",
            "\n",
            "CatBoost:\n",
            "  D: 0.7516778523489933\n",
            "  control: 0.821256038647343\n",
            "\n",
            "Voting Ensemble:\n",
            "  D: 0.7702702702702703\n",
            "  control: 0.8365384615384616\n",
            "\n",
            "Stacking Ensemble:\n",
            "  D: 0.7058823529411765\n",
            "  control: 0.7783251231527094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Сохраняем модель Voting Classifier\n",
        "joblib.dump(voting, 'voting_classifier_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QQ4x0jzGPOv",
        "outputId": "f6315076-97b7-4f2d-ff0b-13b98fcdd911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['voting_classifier_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import spacy\n",
        "from ruts import ReadabilityStats\n",
        "\n",
        "# Загружаем модель spaCy для русского языка\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Функции для расчета метрик\n",
        "def calculate_flesch_kincaid(text):\n",
        "    rs = ReadabilityStats(text)\n",
        "    rs.get_stats()\n",
        "    return rs.get_stats()['flesch_kincaid_grade']\n",
        "\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"аеёиоуыэюя\"\n",
        "    return sum(1 for char in word if char in vowels)\n",
        "\n",
        "def count_complex_words(words):\n",
        "    complex_words = [word for word in words if count_syllables(word) >= 3]\n",
        "    return len(complex_words)\n",
        "\n",
        "def calculate_gunning_fog_index(text):\n",
        "    sentences = re.split(r'[.!?…]', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    words = re.findall(r'\\b[А-Яа-яЁё\\-]+\\b', text)\n",
        "\n",
        "    total_words = len(words)\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    complex_words = count_complex_words(words)\n",
        "\n",
        "    if total_sentences == 0 or total_words == 0:\n",
        "        return 0\n",
        "\n",
        "    gfi = 0.4 * ((total_words / total_sentences) + 100 * (complex_words / total_words))\n",
        "    return round(gfi, 2)\n",
        "\n",
        "def calculate_ttr(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    return unique_words / total_words if total_words != 0 else 0\n",
        "\n",
        "def count_pos(text):\n",
        "    doc = nlp(text)\n",
        "    nouns = [token for token in doc if token.pos_ == 'NOUN']\n",
        "    pronouns = [token for token in doc if token.pos_ == 'PRON']\n",
        "    adverbs = [token for token in doc if token.pos_ == 'ADV']\n",
        "\n",
        "    total_tokens = len([token for token in doc if not token.is_punct and not token.is_space])\n",
        "    return {\n",
        "        'noun_ratio': len(nouns) / total_tokens if total_tokens != 0 else 0,\n",
        "        'pronoun_ratio': len(pronouns) / total_tokens if total_tokens != 0 else 0,\n",
        "        'adverb_ratio': len(adverbs) / total_tokens if total_tokens != 0 else 0\n",
        "    }\n",
        "\n",
        "def calculate_objectivity_coefficient(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    subject_tags = {'NN', 'NNS', 'NNP', 'NNPS',\n",
        "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
        "                    'JJ', 'JJR', 'JJS',\n",
        "                    'RB', 'RBR', 'RBS'}\n",
        "\n",
        "\n",
        "    subject_words = [word for word, tag in tagged_words if tag in subject_tags]\n",
        "\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    meaningful_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    if len(meaningful_words) == 0:\n",
        "        return 0\n",
        "    subjectivity_coefficient = len(subject_words) / len(meaningful_words)\n",
        "\n",
        "    return subjectivity_coefficient\n",
        "\n",
        "def calculate_dynamism_coefficient(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    verb_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
        "    noun_adjective_pronoun_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS', 'PRP', 'PRP$', 'WP', 'WP$'}\n",
        "\n",
        "    verb_count = sum(1 for word, tag in tagged_words if tag in verb_tags)\n",
        "\n",
        "    noun_adjective_pronoun_count = sum(1 for word, tag in tagged_words if tag in noun_adjective_pronoun_tags)\n",
        "\n",
        "    if noun_adjective_pronoun_count == 0:\n",
        "        return float('inf') if verb_count > 0 else 0\n",
        "    dynamism_coefficient = verb_count / noun_adjective_pronoun_count\n",
        "\n",
        "    return dynamism_coefficient\n",
        "\n",
        "# Функция для предсказания на основе модели\n",
        "def predict_text_class(text, model):\n",
        "    # Рассчитываем метрики\n",
        "    flesch_kincaid = calculate_flesch_kincaid(text)\n",
        "    gunning_fog = calculate_gunning_fog_index(text)\n",
        "    ttr = calculate_ttr(text)\n",
        "    pos_counts = count_pos(text)\n",
        "    dynamism = calculate_dynamism_coefficient(text)\n",
        "    objectivity = calculate_objectivity_coefficient(text)\n",
        "\n",
        "    # Подготовка признаков\n",
        "    features = np.array([[flesch_kincaid, gunning_fog, ttr, pos_counts['noun_ratio'], pos_counts['pronoun_ratio'], pos_counts['adverb_ratio'], dynamism, objectivity]])\n",
        "\n",
        "    # Масштабируем признаки\n",
        "    scaler = joblib.load('scaler.pkl')  # Загружаем заранее сохраненный скейлер\n",
        "    features_scaled = scaler.transform(features)\n",
        "\n",
        "    # Получаем предсказание\n",
        "    prediction = model.predict(features_scaled)\n",
        "\n",
        "    if prediction == 0:\n",
        "        return \"Норма\"\n",
        "    else:\n",
        "        return \"Клиническая группа\"\n",
        "\n",
        "# Загружаем модель Voting Classifier\n",
        "model = joblib.load('voting_classifier_model.pkl')\n",
        "\n",
        "# Пример использования\n",
        "user_input = input(\"Введите текст для классификации: \")\n",
        "classification_result = predict_text_class(user_input, model)\n",
        "print(f\"Результат классификации: {classification_result}\")"
      ],
      "metadata": {
        "id": "e7BJXrDfHcZb",
        "outputId": "3b5a67c6-e72b-4ea8-d2f1-9c8e7008bbb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите текст для классификации: Это не гвозди называется, это по-другому как-то называется, я забыла как. Надо взять ножку... надо взять две ножки и прицепить их к основе. Для этого надо использовать два гвоздя. И вставить их в специальные отверстия. Потом мы смотрим на две ножки и там есть еще два отверстия, и мы должны вставить туда палочки. Потом поставить туда еще две ножки, забить туда гвозди, забить гвозди... забить гвозди в палочки и поставить маленький гамак или что это. Завернуть их в палочки, которые расположены между ножками. Всё.\n",
            "Результат классификации: Норма\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}